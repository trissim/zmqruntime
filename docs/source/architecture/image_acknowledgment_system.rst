Image Acknowledgment System
============================

The Problem: Blind Image Processing
------------------------------------

When streaming images to external viewers (Napari, Fiji) during pipeline execution, the main process has no way to know if images are actually being displayed or if viewers have crashed. This creates a "blind" processing situation: the pipeline keeps sending images, but the user can't tell if visualization is working. Additionally, if a viewer crashes or becomes unresponsive, the pipeline continues sending images to a dead process, wasting resources.

The Solution: Acknowledgment-Based Progress Tracking
-----------------------------------------------------

The image acknowledgment system provides real-time tracking of image processing progress in Napari and Fiji viewers. It uses a shared PUSH-PULL ZMQ pattern where all viewers send acknowledgments to a single port (7555) after processing each image. This enables the main process to detect stuck viewers, track progress, and respond to failures.

Overview
--------

The image acknowledgment system provides real-time tracking of image processing progress in Napari and Fiji viewers. It uses a shared PUSH-PULL ZMQ pattern where all viewers send acknowledgments to a single port (7555) after processing each image.

Architecture
------------

Message Flow
~~~~~~~~~~~~

.. code-block:: text

   ┌─────────────────┐
   │  Image Browser  │
   │   (Client)      │
   └────────┬────────┘
            │
            │ 1. Generate UUID for each image
            │ 2. Send images via ZMQ PUB
            │ 3. Register with QueueTracker
            │
            ▼
   ┌─────────────────────────────────────┐
   │  Napari/Fiji Viewer (Server)        │
   │  ┌──────────────────────────────┐  │
   │  │ Receive image on SUB socket  │  │
   │  │ Display in viewer            │  │
   │  │ Send ack on PUSH socket      │  │
   │  └──────────────┬───────────────┘  │
   └─────────────────┼───────────────────┘
                     │
                     │ {image_id, viewer_port, status}
                     ▼
            ┌────────────────┐
            │  Ack Port 7555 │
            │  (PULL socket) │
            └────────┬───────┘
                     │
                     ▼
            ┌────────────────────┐
            │  Ack Listener      │
            │  Routes by port    │
            └────────┬───────────┘
                     │
                     ▼
            ┌────────────────────┐
            │  Queue Tracker     │
            │  mark_processed()  │
            └────────┬───────────┘
                     │
                     ▼
            ┌────────────────────┐
            │  UI Manager        │
            │  Shows progress    │
            └────────────────────┘

Components
----------

ImageAck Message
~~~~~~~~~~~~~~~~

**Location**: ``zmqruntime/messages.py``

.. code-block:: python

   @dataclass(frozen=True)
   class ImageAck:
       """Acknowledgment message sent by viewers after processing an image."""
       image_id: str          # UUID of the processed image
       viewer_port: int       # Port of the viewer that processed it (for routing)
       viewer_type: str       # 'napari' or 'fiji'
       status: str = 'success'  # 'success', 'error', etc.
       timestamp: float = None  # When it was processed
       error: str = None      # Error message if status='error'

**Message Format** (JSON over ZMQ):

.. code-block:: json

   {
       "type": "image_ack",
       "image_id": "550e8400-e29b-41d4-a716-446655440000",
       "viewer_port": 5555,
       "viewer_type": "napari",
       "status": "success",
       "timestamp": 1234567890.123
   }

QueueTracker
~~~~~~~~~~~~

**Location**: ``zmqruntime/queue_tracker.py`` (OpenHCS shim: ``openhcs/runtime/queue_tracker.py``)

Tracks pending and processed images for a single viewer.

.. code-block:: python

   class QueueTracker:
       """Thread-safe tracker for a single viewer's image queue."""
       
       def __init__(self, viewer_port: int, viewer_type: str, timeout_seconds: float = 30.0):
           self.viewer_port = viewer_port
           self.viewer_type = viewer_type
           self.timeout_seconds = timeout_seconds
           self._pending: Dict[str, float] = {}  # {image_id: timestamp_sent}
           self._processed: Set[str] = set()
       
       def register_sent(self, image_id: str):
           """Register that an image was sent to the viewer."""
           self._pending[image_id] = time.time()
           self._total_sent += 1
       
       def mark_processed(self, image_id: str):
           """Mark image as processed (ack received)."""
           if image_id in self._pending:
               del self._pending[image_id]
               self._processed.add(image_id)
               self._total_processed += 1
       
       def get_progress(self) -> Tuple[int, int]:
           """Get (processed_count, total_sent_count)."""
           return (self._total_processed, self._total_sent)
       
       def has_stuck_images(self) -> bool:
           """Check if any images have been pending > timeout_seconds."""
           now = time.time()
           return any(now - sent_time > self.timeout_seconds 
                     for sent_time in self._pending.values())

GlobalQueueTrackerRegistry
~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Location**: ``zmqruntime/queue_tracker.py`` (OpenHCS shim: ``openhcs/runtime/queue_tracker.py``)

Singleton that manages queue trackers for all active viewers.

.. code-block:: python

   class GlobalQueueTrackerRegistry:
       """Global registry of queue trackers (singleton)."""
       
       def get_or_create_tracker(self, viewer_port: int, viewer_type: str) -> QueueTracker:
           """Get existing tracker or create new one."""
           if viewer_port not in self._trackers:
               self._trackers[viewer_port] = QueueTracker(viewer_port, viewer_type)
           return self._trackers[viewer_port]
       
       def get_tracker(self, viewer_port: int) -> Optional[QueueTracker]:
           """Get tracker for a viewer port."""
           return self._trackers.get(viewer_port)
       
       def get_all_trackers(self) -> Dict[int, QueueTracker]:
           """Get all active trackers."""
           return dict(self._trackers)

Global Ack Listener
~~~~~~~~~~~~~~~~~~~

**Location**: ``zmqruntime/ack_listener.py`` (OpenHCS uses a shim in ``openhcs/runtime/zmq_base.py``)

Singleton thread that listens on port 7555 for acks from all viewers.

.. code-block:: python

   SHARED_ACK_PORT = 7555
   
   def start_global_ack_listener():
       """Start the global ack listener thread (singleton)."""
       # Creates daemon thread running _ack_listener_loop()
       # Safe to call multiple times - only starts once
   
   def _ack_listener_loop():
       """Main loop for ack listener thread."""
       registry = GlobalQueueTrackerRegistry()
       socket = zmq.Context().socket(zmq.PULL)
       socket.bind(f"tcp://*:{SHARED_ACK_PORT}")
       
       while _ack_listener_running:
           if socket.poll(timeout=1000):
               ack_dict = socket.recv_json()
               ack = ImageAck.from_dict(ack_dict)
               
               # Route to appropriate queue tracker
               tracker = registry.get_tracker(ack.viewer_port)
               if tracker:
                   tracker.mark_processed(ack.image_id)

**Startup**: Called once in ``ImageBrowserWidget.__init__()``

Viewer Implementation
---------------------

Napari Viewer
~~~~~~~~~~~~~

**Location**: ``openhcs/runtime/napari_stream_visualizer.py``

.. code-block:: python

   class NapariViewerServer(ZMQServer):
       def __init__(self, port, ...):
           # Create PUSH socket for acks
           self.ack_socket = zmq.Context().socket(zmq.PUSH)
           self.ack_socket.connect(f"tcp://localhost:{SHARED_ACK_PORT}")
       
       def _process_single_image(self, image_info, display_config):
           image_id = image_info.get('image_id')
           
           try:
               # Load and display image
               image_data = load_from_shared_memory(...)
               display_in_napari(image_data)
               
               # Send success ack
               if image_id:
                   self._send_ack(image_id, status='success')
           
           except Exception as e:
               # Send error ack
               if image_id:
                   self._send_ack(image_id, status='error', error=str(e))
               raise
       
       def _send_ack(self, image_id, status='success', error=None):
           ack = ImageAck(
               image_id=image_id,
               viewer_port=self.port,
               viewer_type='napari',
               status=status,
               timestamp=time.time(),
               error=error
           )
           self.ack_socket.send_json(ack.to_dict())

Fiji Viewer
~~~~~~~~~~~

**Location**: ``openhcs/runtime/fiji_viewer_server.py``

.. code-block:: python

   class FijiViewerServer(ZMQServer):
       def __init__(self, port, ...):
           # Create PUSH socket for acks
           self.ack_socket = zmq.Context().socket(zmq.PUSH)
           self.ack_socket.connect(f"tcp://localhost:{SHARED_ACK_PORT}")
       
       def _add_images_to_hyperstack(self, images, ...):
           # Build hyperstack from images
           hyperstack = build_hyperstack(images)
           hyperstack.show()
           
           # Send acks for all successfully displayed images
           for img_data in images:
               image_id = img_data.get('image_id')
               if image_id:
                   self._send_ack(image_id, status='success')

Client Implementation
---------------------

Streaming Backends
~~~~~~~~~~~~~~~~~~

**Locations**: 
- ``openhcs/io/napari_stream.py``
- ``openhcs/io/fiji_stream.py``

.. code-block:: python

   class NapariStreamingBackend(StreamingBackend):
       def save_batch(self, data_list, file_paths, **kwargs):
           napari_port = kwargs['napari_port']
           batch_images = []
           image_ids = []
           
           # Generate UUID for each image
           for data, file_path in zip(data_list, file_paths):
               image_id = str(uuid.uuid4())
               image_ids.append(image_id)
               
               # Create shared memory and metadata
               batch_images.append({
                   'path': str(file_path),
                   'shm_name': shm_name,
                   'image_id': image_id  # Include UUID
               })
           
           # Send batch to viewer
           publisher.send_json({'images': batch_images})
           
           # Register sent images with queue tracker
           registry = GlobalQueueTrackerRegistry()
           tracker = registry.get_or_create_tracker(napari_port, 'napari')
           for image_id in image_ids:
               tracker.register_sent(image_id)

UI Integration
--------------

See :doc:`../guides/viewer_management` for UI usage.

Port Allocation
---------------

The acknowledgment system uses the following port allocation:

- **Napari viewers**: 5555-5564 (10 ports)
- **Fiji viewers**: 5565-5574 (10 ports)
- **Shared ack port**: 7555 (all viewers)
- **Execution server**: 7777

**Control ports**: Data port + 1000 (e.g., 5555 → 6555)

Performance
-----------

**Overhead**:

- UUID generation: ~1μs per image
- Ack message size: ~150 bytes JSON
- Network latency: <1ms localhost
- Queue tracker lookup: O(1) hash table

**Scalability**:

- Tested with 10 concurrent viewers
- Handles 1000+ images/second throughput
- Thread-safe for multi-viewer scenarios
- No blocking on image display path

Error Handling
--------------

**Stuck Image Detection**:

Images pending >30s without ack are marked as stuck:

.. code-block:: python

   if tracker.has_stuck_images():
       stuck = tracker.get_stuck_images()
       # Returns: [(image_id, elapsed_seconds), ...]

**Error Acks**:

Viewers send error acks if processing fails:

.. code-block:: json

   {
       "type": "image_ack",
       "image_id": "...",
       "status": "error",
       "error": "Failed to read shared memory: ..."
   }

**Fail-Loud Philosophy**:

- Errors are logged and propagated
- No silent failures
- UI shows stuck/error status clearly
